{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pprint\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Looking_VVG Good_AJ0 Feeling_VVG Good_AJ0 book_NN1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f = open(f\"./train_processed.txt\", \"r\")\n",
    "#sentences = f.readlines()\n",
    "#f.close()\n",
    "#print(len(sentences))\n",
    "\n",
    "#sentences = [sentence.rstrip() for sentence in sentences if len(sentence) > 2]\n",
    "#sentences[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8899819\n",
      "by_PRP\n"
     ]
    }
   ],
   "source": [
    "#word_tag_pairs = []\n",
    "#for sentence in sentences:\n",
    "    #for word_tag_pair in sentence.split(\" \"):\n",
    "        #if len(word_tag_pair.strip()) != 0:\n",
    "         #   word_tag_pairs.append(word_tag_pair.strip())\n",
    "#print(len(word_tag_pairs))\n",
    "#print(word_tag_pairs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "per\n"
     ]
    }
   ],
   "source": [
    "#for pair in word_tag_pairs:\n",
    " #   try:\n",
    "  #      tag = pair.split(\"_\")[1]\n",
    "   # except:\n",
    "    #    print(pair)\n",
    "     #   word_tag_pairs.remove(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words = [pair.split('_')[0] for pair in word_tag_pairs]\n",
    "#all_tags = [pair.split('_')[1] for pair in word_tag_pairs]\n",
    "#word_freq = Counter(all_words).most_common() # represent how many times each word appeared in dataset\n",
    "#tag_freq = Counter(all_tags).most_common() # represent how many times each tag appeared in dataset\n",
    "#train_words = [word[0] for word in word_freq] # all the words in the dataset\n",
    "#tags = [word[0] for word in tag_freq] # all the tags in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./train_words.txt\", \"w+\") as f:\n",
    " #   for word in train_words:\n",
    "  #      f.write(f\"{word}\\n\")\n",
    "   # f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NN1', 1: 'AT0', 2: 'PRP', 3: 'AJ0', 4: 'NN2', 5: 'NP0', 6: 'AV0', 7: 'PNP', 8: 'CJC', 9: 'PRF', 10: 'VVI', 11: 'DT0', 12: 'VVN', 13: 'VVD', 14: 'CRD', 15: 'TO0', 16: 'DPS', 17: 'VM0', 18: 'VBD', 19: 'VVG', 20: 'CJS', 21: 'VBZ', 22: 'VVB', 23: 'VVZ', 24: 'POS', 25: 'XX0', 26: 'CJT', 27: 'AVP', 28: 'VBI', 29: 'DTQ', 30: 'NN0', 31: 'VBB', 32: 'NN1-VVB', 33: 'AJ0-NN1', 34: 'VHD', 35: 'ORD', 36: 'NP0-NN1', 37: 'VHZ', 38: 'VHB', 39: 'VBN', 40: 'PNI', 41: 'PNQ', 42: 'NN1-AJ0', 43: 'PRP-AVP', 44: 'EX0', 45: 'VVN-VVD', 46: 'AJC', 47: 'AVQ', 48: 'VVB-NN1', 49: 'VVD-VVN', 50: 'VHI', 51: 'NN1-NP0', 52: 'VVG-AJ0', 53: 'PNX', 54: 'UNC', 55: 'AJS', 56: 'NN2-VVZ', 57: 'VDB', 58: 'VDD', 59: 'VVG-NN1', 60: 'VVN-AJ0', 61: 'VBG', 62: 'AVP-PRP', 63: 'CJT-DT0', 64: 'AV0-AJ0', 65: 'NN1-VVG', 66: 'AJ0-AV0', 67: 'ITJ', 68: 'VVZ-NN2', 69: 'VDI', 70: 'PRP-CJS', 71: 'AJ0-VVG', 72: 'CJS-AVQ', 73: 'ZZ0', 74: 'VDZ', 75: 'CJS-PRP', 76: 'AJ0-VVN', 77: 'DT0-CJT', 78: 'VHG', 79: 'AJ0-VVD', 80: 'VDN', 81: 'VHN', 82: 'AVQ-CJS', 83: 'VDG', 84: 'VVD-AJ0', 85: 'PNI-CRD', 86: 'CRD-PNI', 88: 'start'}\n"
     ]
    }
   ],
   "source": [
    "#tags_to_index = {}\n",
    "#for index in range(len(tag_freq)):\n",
    "#    tags_to_index[tag_freq[index][0]] = index\n",
    "#print(tags_to_index)\n",
    "#tags_to_index[\"start\"] = len(tags_to_index) #last tag is start tag\n",
    "#np.save(\"tag_to_index.npy\", tags_to_index)\n",
    "\n",
    "#index_to_tags = {}\n",
    "#for index in range(len(tag_freq)):\n",
    " #   index_to_tags[index] = tag_freq[index][0]\n",
    "#index_to_tags[len(tags_to_index)] = \"start\" #last tag is start tag\n",
    "#np.save(\"index_to_tag.npy\", index_to_tags)\n",
    "#print(index_to_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The_AT0 Security_NN1 Service_NN1 is_VBZ part_NN1 of_PRF the_AT0 Defence_NN1 Forces_NN2 of_PRF the_AT0 country_NN1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(f\"./test_processed.txt\", \"r\")\n",
    "sentences = f.readlines()\n",
    "f.close()\n",
    "\n",
    "sentences = [sentence.rstrip() for sentence in sentences if len(sentence) > 2]\n",
    "print(len(sentences))\n",
    "sentences[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NN1', 1: 'AT0', 2: 'PRP', 3: 'AJ0', 4: 'NN2', 5: 'NP0', 6: 'AV0', 7: 'PNP', 8: 'CJC', 9: 'PRF', 10: 'VVI', 11: 'VVN', 12: 'DT0', 13: 'VVD', 14: 'CRD', 15: 'TO0', 16: 'DPS', 17: 'VVG', 18: 'VM0', 19: 'VBD', 20: 'CJS', 21: 'VBZ', 22: 'VVB', 23: 'VVZ', 24: 'CJT', 25: 'AVP', 26: 'POS', 27: 'XX0', 28: 'VBI', 29: 'DTQ', 30: 'NN0', 31: 'VBB', 32: 'VHD', 33: 'ORD', 34: 'NN1-VVB', 35: 'VHZ', 36: 'AJ0-NN1', 37: 'VHB', 38: 'PNI', 39: 'NP0-NN1', 40: 'VBN', 41: 'PNQ', 42: 'EX0', 43: 'AVQ', 44: 'AJC', 45: 'VHI', 46: 'PRP-AVP', 47: 'NN1-AJ0', 48: 'VVN-VVD', 49: 'VVD-VVN', 50: 'VVB-NN1', 51: 'PNX', 52: 'UNC', 53: 'AJS', 54: 'NN1-NP0', 55: 'VDB', 56: 'VDD', 57: 'VVG-AJ0', 58: 'VBG', 59: 'NN2-VVZ', 60: 'VVG-NN1', 61: 'VVN-AJ0', 62: 'ITJ', 63: 'VDI', 64: 'AVP-PRP', 65: 'AV0-AJ0', 66: 'CJT-DT0', 67: 'ZZ0', 68: 'VDZ', 69: 'NN1-VVG', 70: 'AJ0-AV0', 71: 'VVZ-NN2', 72: 'AJ0-VVG', 73: 'PRP-CJS', 74: 'CJS-AVQ', 75: 'AJ0-VVN', 76: 'CJS-PRP', 77: 'VHG', 78: 'DT0-CJT', 79: 'VDN', 80: 'VHN', 81: 'AJ0-VVD', 82: 'VDG', 83: 'AVQ-CJS', 84: 'VVD-AJ0', 85: 'PNI-CRD', 86: 'CRD-PNI', 88: 'start'}\n"
     ]
    }
   ],
   "source": [
    "index_to_tags = np.load(\"index_to_tag.npy\",allow_pickle=True).item()\n",
    "tags_to_index = np.load(\"tag_to_index.npy\",allow_pickle=True).item()\n",
    "print(index_to_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5377\n",
      "422\n"
     ]
    }
   ],
   "source": [
    "em_probs = np.load(\"word_given_tag.npy\",allow_pickle=True).item()\n",
    "tr_probs = np.load(\"Trans_Matrix.npy\",allow_pickle=True)\n",
    "\n",
    "print(em_probs[\"VVI\"][\"make\"])\n",
    "print(em_probs[\"VVI\"][\"encourage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199464/199464 [06:39<00:00, 499.83it/s] \n"
     ]
    }
   ],
   "source": [
    "gt_tags = []\n",
    "pred_tags = []\n",
    "countx=0\n",
    "\n",
    "for sentence in tqdm(sentences):\n",
    "    prev_tag = \"start\"\n",
    "    \n",
    "    for word_tag_pair_idx in range(len(sentence.split(\" \"))):\n",
    "        word_tag_pair = sentence.split(\" \")[word_tag_pair_idx]\n",
    "        if len(word_tag_pair.strip()) != 0:\n",
    "            #print(word_tag_pair)\n",
    "            word = word_tag_pair.split(\"_\")[0]\n",
    "            gt_tag = tags_to_index[word_tag_pair.split(\"_\")[1]]\n",
    "    \n",
    "            possible_tags = []\n",
    "            for idx in range(len(index_to_tags)):\n",
    "                try:\n",
    "                    emprob_for_this_word_tag = em_probs[index_to_tags[idx]][word]\n",
    "                except:\n",
    "                    emprob_for_this_word_tag = 0\n",
    "\n",
    "                tr_prob = tr_probs[tags_to_index[prev_tag], idx]\n",
    "\n",
    "                possible_tags.append(emprob_for_this_word_tag*tr_prob)\n",
    "\n",
    "            possible_tags = np.array(possible_tags)\n",
    "            if np.max(possible_tags) == 0:\n",
    "                predicted_tag = index_to_tags[np.argmax(tr_probs[tags_to_index[prev_tag]])]\n",
    "            else:\n",
    "                predicted_tag = index_to_tags[np.argmax(possible_tags)]\n",
    "            \n",
    "#             total_pred += 1\n",
    "#             if (tags_to_index[predicted_tag] == gt_tag):\n",
    "#                 correct_pred += 1\n",
    "            gt_tags.append(gt_tag)\n",
    "            pred_tags.append(tags_to_index[predicted_tag])\n",
    "            if predicted_tag in word_tag_pair.split(\"_\")[1]:\n",
    "                countx+=1\n",
    "\n",
    "            prev_tag = predicted_tag\n",
    "            #print(f\"{predicted_tag}, {index_to_tags[gt_tag]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 91.25108850614026%\n",
      "The total data has  = 3528919\n",
      "The no of correct predictions = 3220177\n",
      "The no of incorrect predictions = 308742\n"
     ]
    }
   ],
   "source": [
    "#count_cp = 0\n",
    "#for index in range(len(gt_tags)):\n",
    " #   if pred_tags[index] == gt_tags[index]:\n",
    "        # correct prediction\n",
    "  #      count_cp += 1\n",
    "\n",
    "accuracy = countx/len(gt_tags)\n",
    "print(f\"Accuracy = {accuracy*100}%\")\n",
    "print(f\"The total data has  = {len(gt_tags)}\")\n",
    "print(f\"The no of correct predictions = {countx}\")\n",
    "print(f\"The no of incorrect predictions = {len(gt_tags)-countx}\")\n",
    "#print(countx/len(gt_tags))\n",
    "#print(gt_tags[9])\n",
    "#print(pred_tags[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The: AT0\n",
      "Security: NN1\n",
      "Service: NN1\n",
      "is: VBZ\n",
      "part: NN1\n",
      "of: PRF\n",
      "the: AT0\n",
      "Defence: NN1\n",
      "Forces: NN2\n",
      "of: PRF\n",
      "the: AT0\n",
      "country: NN1\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"The Security Service is part of the Defence Forces of the country\"\n",
    "\n",
    "for word_tag_pair_idx in range(len(new_sentence.split(\" \"))):\n",
    "        word_tag_pair = new_sentence.split(\" \")[word_tag_pair_idx]\n",
    "        word = word_tag_pair.split(\"_\")[0]\n",
    "\n",
    "        possible_tags = []\n",
    "        for idx in range(len(index_to_tags)):\n",
    "            try:\n",
    "                emprob_for_this_word_tag = em_probs[index_to_tags[idx]][word]\n",
    "            except:\n",
    "                emprob_for_this_word_tag = 0\n",
    "\n",
    "            tr_prob = tr_probs[tags_to_index[prev_tag], idx]\n",
    "\n",
    "            possible_tags.append(emprob_for_this_word_tag*tr_prob)\n",
    "\n",
    "        possible_tags = np.array(possible_tags)\n",
    "        if np.max(possible_tags) == 0:\n",
    "            predicted_tag = \"UNC\"\n",
    "        else:\n",
    "            predicted_tag = index_to_tags[np.argmax(possible_tags)]\n",
    "\n",
    "        print(f\"{word}: {predicted_tag}\")\n",
    "        prev_tag = predicted_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[512224   2051   1395 ...      0      0      0]\n",
      " [    23 337605      0 ...      0      0      0]\n",
      " [   252      9 277520 ...      0      0      0]\n",
      " ...\n",
      " [    13     11      4 ...      0      0      0]\n",
      " [     0      0      0 ...      0      0      0]\n",
      " [     0      0      0 ...      0      0      0]]\n",
      "30060\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "\n",
    "cm = confusion_matrix(gt_tags, pred_tags)\n",
    "np.savetxt(\"hmm_confusion_matrix.txt\", cm, fmt=\"%d\", delimiter=\"\\t\")\n",
    "print(cm)\n",
    "print(cm[20,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
